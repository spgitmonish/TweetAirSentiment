```{r, warning=FALSE}
library(mlbench)
library(caret)
library(ggplot2)
library(tm)
library(wordcloud)
```

## Analyze Data, clean up if necessary
```{r, warning=FALSE}
dataset <- read.csv("Tweets.csv")

# Look at the dimensions of the dataset
dim(dataset)

# Look at the first view variables of the dataset
head(dataset)

# Class of each of the variables
sapply(dataset, class)

# Remove the "tweet_id", "tweet_created", "tweet_coord", airline_sentiment_gold"" & "negativereason_gold" variables
# These variables don't have enough values or don't add any value to the analysis
dataset <- subset(dataset, select = -c(tweet_id, tweet_created, tweet_coord, 
                                       airline_sentiment_gold, negativereason_gold)) 

# Summary of the data
summary(dataset)

# Look at the class distribution of the dependent variable
cbind(freq=table(dataset$airline_sentiment), percentage=prop.table(table(dataset$airline_sentiment))*100)
```
After looking at the data it is clear that the sentiment is Negative: 62.6%, Neutral: 21.2% & Positive: 16.1%. Not good for airlines unfortunately. This is just from the initial analysis.

## Data Visualization
```{r, echo=FALSE}
# Bar plot of each airline seperated by sentiment
ggplot(data = dataset, aes(x = airline, fill = airline_sentiment)) +
  geom_bar(stat = "count")

# Look at the distribution of airline_sentiment_confidence
ggplot(data = dataset, aes(x = airline_sentiment_confidence, fill = airline_sentiment)) + 
  geom_histogram(bins = 50) + 
  facet_grid(airline_sentiment ~ airline, scales = "free_y") + 
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 80))

# Look at the distribution of airline_sentiment_confidence
ggplot(data = dataset, aes(x = airline_sentiment_confidence, fill = airline_sentiment)) + 
  geom_histogram(bins = 50) + 
  facet_grid(airline_sentiment ~ airline, scales = "free_y") + 
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 80))
```

## Text analysis
```{r, warning=FALSE}
# Concatenate vectors after converting to characters(collapse says we want to collapse elements
# of the vector rather than pasting vectors together)
tweet_text <- paste(dataset$text, collapse = " ")

# Create a vector source from the previously created vector
tweet_source <- VectorSource(tweet_text)

# Create a corpus(a collection of text)
corpus <- Corpus(tweet_source)

# Clean text: Convert to lower case, remove punctuation, strip white space &
#             remove stop words(common words)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))

# Create the document term matrix from the corpus
tweet_dtm <- DocumentTermMatrix(corpus)
tweet_dtm <- as.matrix(tweet_dtm)

# Take the column sums of this matrix 
frequency <- colSums(tweet_dtm)

# Sort by the most frequently used words
frequency <- sort(frequency, decreasing = TRUE)

# Get a list of the words
words <- names(frequency)

# Plot the top 100 words in our cloud
wordcloud(words[1:100], frequency[1:100])

# After looking at the word cloud remove the words which represent airlines.
# These words represent the twitter handles
corpus <- tm_map(corpus, removeWords, c("americanair", "virginamerica", "southwestair",
                                            "jetblue", "united", "usairways"))

# Recreate the word cloud after removal
tweet_dtm <- DocumentTermMatrix(corpus)
tweet_dtm <- as.matrix(tweet_dtm)
frequency <- colSums(tweet_dtm)
frequency <- sort(frequency, decreasing = TRUE)
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
```

## Create a Validation Dataset
```{r, warning=FALSE}
# Split data set with 80% for training and 20% for validation
validationIndex <- createDataPartition(dataset$airline_sentiment, p = 0.80, list = FALSE)

# Select 20% of the data for validation
validation <- dataset[-validationIndex,]

# Use the remaining 80% of the data for training and testing the models
dataset <- dataset[validationIndex,]
```
