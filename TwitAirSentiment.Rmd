```{r, warning=FALSE}
library(mlbench)
library(caret)
library(ggplot2)
library(tm)
library(wordcloud)
library(SparseM)
library(e1071)
```

## Analyze Data, clean up if necessary
```{r, warning=FALSE}
dataset <- read.csv("Tweets.csv")

# Look at the dimensions of the dataset
dim(dataset)

# Look at the first view variables of the dataset
head(dataset)

# Class of each of the variables
sapply(dataset, class)

# Remove the "tweet_id", "tweet_created", "tweet_coord", airline_sentiment_gold"" & "negativereason_gold" variables
# These variables don't have enough values or don't add any value to the analysis
dataset <- subset(dataset, select = -c(tweet_id, tweet_created, tweet_coord, 
                                       airline_sentiment_gold, negativereason_gold)) 

# Summary of the data
summary(dataset)

# Look at the class distribution of the dependent variable
cbind(freq=table(dataset$airline_sentiment), percentage=prop.table(table(dataset$airline_sentiment))*100)
```

After looking at the data it is clear that the sentiment is Negative: 62.6%, Neutral: 21.2% & Positive: 16.1%. Not good for airlines unfortunately. This is just from the initial analysis.

## Data Visualization
```{r, echo=FALSE}
# Bar plot of each airline seperated by sentiment
ggplot(data = dataset, aes(x = airline, fill = airline_sentiment)) +
  geom_bar(stat = "count")

# Look at the distribution of airline_sentiment_confidence
ggplot(data = dataset, aes(x = airline_sentiment_confidence, fill = airline_sentiment)) + 
  geom_histogram(bins = 50) + 
  facet_grid(airline_sentiment ~ airline, scales = "free_y") + 
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 80))

# Look at the distribution of airline_sentiment_confidence
ggplot(data = dataset, aes(x = airline_sentiment_confidence, fill = airline_sentiment)) + 
  geom_histogram(bins = 50) + 
  facet_grid(airline_sentiment ~ airline, scales = "free_y") + 
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 80))
```

## Text analysis for the actual tweet
```{r, warning=FALSE}
# Concatenate vectors after converting to characters(collapse says we want to collapse elements
# of the vector rather than pasting vectors together)
tweet_text <- paste(dataset$text, collapse = " ")

# Create a vector source from the previously created vector
tweet_source <- VectorSource(tweet_text)

# Create a corpus(a collection of text)
corpus <- Corpus(tweet_source)

# Clean text: Convert to lower case, remove punctuation, strip white space &
#             remove stop words(common words)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))

# Create the document term matrix from the corpus
tweet_dtm <- DocumentTermMatrix(corpus)
tweet_dtm <- as.matrix(tweet_dtm)

# Take the column sums of this matrix 
frequency <- colSums(tweet_dtm)

# Sort by the most frequently used words
frequency <- sort(frequency, decreasing = TRUE)

# Get a list of the words
words <- names(frequency)

# Plot the top 100 words in our cloud
wordcloud(words[1:100], frequency[1:100])

# After looking at the word cloud remove the words which represent airlines.
# These words represent the twitter handles
corpus <- tm_map(corpus, removeWords, c("americanair", "virginamerica", "southwestair",
                                            "jetblue", "united", "usairways"))

# Recreate the word cloud after removal
tweet_dtm <- DocumentTermMatrix(corpus)
tweet_dtm <- as.matrix(tweet_dtm)
frequency <- colSums(tweet_dtm)
frequency <- sort(frequency, decreasing = TRUE)
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
```

## Create a Validation Dataset
```{r, warning=FALSE}
# Split data set with 80% for training and 20% for validation
set.seed(7)
validationIndex <- createDataPartition(dataset$airline_sentiment, p = 0.80, list = FALSE)

# Select 20% of the data for validation
validation <- dataset[-validationIndex,]

# Use the remaining 80% of the data for training and testing the models
dataset <- dataset[validationIndex,]
```

## Text analysis of the negative reason
```{r, warning=FALSE}
# Concatenate vectors after converting to characters(collapse says we want to collapse elements
# of the vector rather than pasting vectors together)
reason_text <- paste(dataset$negativereason, collapse = " ")

# Create a vector source from the previously created vector
reason_source <- VectorSource(reason_text)

# Create a corpus(a collection of text)
reason_corpus <- Corpus(reason_source)

# Clean text: Convert to lower case, remove punctuation, strip white space &
#             remove stop words(common words)
reason_corpus <- tm_map(reason_corpus, content_transformer(tolower))
reason_corpus <- tm_map(reason_corpus, removePunctuation)
reason_corpus <- tm_map(reason_corpus, stripWhitespace)
reason_corpus <- tm_map(reason_corpus, removeWords, stopwords("en"))

# Create the document term matrix from the corpus
reason_dtm <- DocumentTermMatrix(reason_corpus)
reason_dtm <- as.matrix(reason_dtm)

# Take the column sums of this matrix 
reason_frequency <- colSums(reason_dtm)

# Sort by the most frequently used words
reason_frequency <- sort(reason_frequency, decreasing = TRUE)

# Get a list of the words
reason_words <- names(reason_frequency)

# Plot the top 100 words in our cloud
wordcloud(reason_words, reason_frequency)
```

## Naive Bayes & SVM Text Classification
```{r, warning=FALSE}
# Convert the training and validation data sets to data frames
traindf <- as.data.frame(dataset)
testdf <- as.data.frame(validation)

# Create a corpus for training and validation data frames
text_trainvector <- as.vector(traindf$text)
text_testvector <- as.vector(testdf$text)

# Create source for the vectors
text_trainsource <- VectorSource(text_trainvector)
text_testsource <- VectorSource(text_testvector)

# Create corpus for the data
text_traincorpus <- Corpus(text_trainsource)
text_testcorpus <- Corpus(text_testsource)

# Repeat the steps from the previous section(cleaning)
text_traincorpus <- tm_map(text_traincorpus, content_transformer(tolower))
text_traincorpus <- tm_map(text_traincorpus, removePunctuation)
text_traincorpus <- tm_map(text_traincorpus, stripWhitespace)
text_traincorpus <- tm_map(text_traincorpus, removeWords, stopwords("en"))
text_traincorpus <- tm_map(text_traincorpus, removeWords, 
                           c("americanair", "virginamerica", "southwestair",
                             "jetblue", "united", "usairways"))

text_testcorpus <- tm_map(text_testcorpus, content_transformer(tolower))
text_testcorpus <- tm_map(text_testcorpus, removePunctuation)
text_testcorpus <- tm_map(text_testcorpus, stripWhitespace)
text_testcorpus <- tm_map(text_testcorpus, removeWords, stopwords("en"))
text_testcorpus <- tm_map(text_testcorpus, removeWords, c("americanair", "virginamerica",
                                                          "southwestair","jetblue", "united",
                                                          "usairways"))

# Create the term document matrix
text_trainmatrix <- t(TermDocumentMatrix(text_traincorpus))
text_testmatrix <- t(TermDocumentMatrix(text_testcorpus))

# Using Naive Bayes and airline_sentiment as the class variable train the classifier
set.seed(7)
text_modelnb <- naiveBayes(as.matrix(text_trainmatrix), 
                           as.factor(traindf$airline_sentiment))

# Using SVM with radial bias and airline_sentiment as the class variable train the classifier
set.seed(7)
text_modelsvm <- svm(x = as.matrix(text_trainmatrix), y = traindf$airline_sentiment, 
                     kernel = "radial", k = 10)

# Predict!
text_nbResults <- predict(text_modelnb, as.matrix(text_testmatrix))
text_svmResults <- predict(reason_modelsvm, as.matrix(reason_testmatrix))

# Compare results
confusionMatrix(text_nbResults, testdf$airline_sentiment)
confusionMatrix(text_svmResults, testdf$airline_sentiment)
```

## Naive Bayes & SVM Negative Reason Classification
```{r, warning=FALSE}
# Create a corpus for training and validation data frames
reason_trainvector <- as.vector(traindf$negativereason)
reason_testvector <- as.vector(testdf$negativereason)

# Create source for the vectors
reason_trainsource <- VectorSource(reason_trainvector)
reason_testsource <- VectorSource(reason_testvector)

# Create corpus for the data
reason_traincorpus <- Corpus(reason_trainsource)
reason_testcorpus <- Corpus(reason_testsource)

# Repeat the steps from the previous section(cleaning)
reason_traincorpus <- tm_map(reason_traincorpus, content_transformer(tolower))
reason_traincorpus <- tm_map(reason_traincorpus, removePunctuation)
reason_traincorpus <- tm_map(reason_traincorpus, stripWhitespace)
reason_traincorpus <- tm_map(reason_traincorpus, removeWords, stopwords("en"))

reason_testcorpus <- tm_map(reason_testcorpus, content_transformer(tolower))
reason_testcorpus <- tm_map(reason_testcorpus, removePunctuation)
reason_testcorpus <- tm_map(reason_testcorpus, stripWhitespace)
reason_testcorpus <- tm_map(reason_testcorpus, removeWords, stopwords("en"))

# Create the term document matrix
reason_trainmatrix <- t(TermDocumentMatrix(reason_traincorpus))
reason_testmatrix <- t(TermDocumentMatrix(reason_testcorpus))

# Using Naive Bayes and airline_sentiment as the class variable train the classifier
set.seed(7)
reason_modelnb <- naiveBayes(as.matrix(reason_trainmatrix), traindf$airline_sentiment)

# Using SVM with radial bias and airline_sentiment as the class variable train the classifier
set.seed(7)
reason_modelsvm <- svm(x = as.matrix(reason_trainmatrix), y = traindf$airline_sentiment, 
                       kernel = "radial", k = 10)

# Predict!
reason_nbResults <- predict(reason_modelnb, as.matrix(reason_testmatrix))
reason_svmResults <- predict(reason_modelsvm, as.matrix(reason_testmatrix))

# Compare results
confusionMatrix(reason_nbResults, testdf$airline_sentiment)
confusionMatrix(reason_svmResults, testdf$airline_sentiment)
```